Nice problem to have: you’ve got a workflow builder, now you need a toil radar for the team’s brains. Let’s set that up.

I’ll give you:

1. A “Toil Discovery” question set your team can use to surface candidates.


2. A Toil Qualification Checklist + simple scoring so people don’t argue endlessly about “is this really toil?”.


3. A lightweight documentation template that’s easy to convert into a workflow spec.



I’ll wrap it in the SREPro+ structure you like.


---

1. Solution – Ready-to-use framework

A. Toil Discovery Question Set

Use this in a 30–45 min session (or async form) for each engineer. Ask them to scroll through their last 2–4 weeks of tickets / pages / chats while answering.

1) Repetitive operational work

In the last month, what did you do 3+ times that looked almost identical (same commands/clicks/queries)?

Which tasks did you do where your brain mostly did: “copy from runbook, paste into terminal / Splunk / browser”?

Which tasks could you hand off to a junior tomorrow with a checklist and they’d be fine?


2) Ticket & incident patterns

Which incident types keep coming back with almost the same resolution steps?

Which Jira/ServiceNow ticket categories are most common for your app? What % of them follow a standard path?

For which incidents do you:

Check 2–3 dashboards or Splunk queries

Decide “yep, known issue”

Apply a standard fix or file a standard upstream ticket?



3) Consumer / stakeholder requests

What are the top 5 questions consumers (internal users, other teams, customers) repeatedly ask you?

“Can you fetch X logs?”

“Can you check if job Y succeeded?”

“Can you share metrics/report for Z?”


Which requests are basically data lookups:

Read from an HTTP endpoint

Scrape a status/report web page

Run a Splunk search and translate the result?



4) Manual checks & health verification

What do you check before/after deployments, manually, almost every time?

What daily/weekly health checks do you perform that follow a checklist?

Where do you:

Open a web UI to see a status

Run a Splunk query

Hit an internal API
Always in the same order?



5) Reporting & status communication

What recurring reports or summaries do you create for managers/consumers?

Which of those are generated by:

Running fixed queries

Copying numbers/graphs

Writing a short interpretation that is mostly boilerplate?



6) “Glue work” & coordination

When something breaks, what coordination actions do you repeat?

Creating similar incident tickets

Pinging the same Slack channels with a standard template

Updating a status page with structured info


Could a workflow pre-fill 80% of that info if it had:

Endpoint data

Logs from Splunk

A web page scrape?




---

B. Toil Qualification Checklist (Is this worth automating?)

For each candidate task from above, run these questions:

Toil definition (Google SRE–style)
Toil tends to be: manual, repetitive, automatable, tactical, no enduring value, and grows with service size.

Have engineers answer Yes/No:

1. Manual – Do you have to do this by hand every time (not already a script/cron)?


2. Repetitive – Did you do this ≥ 3 times in the last month?


3. Low cognitive load – Do you mostly follow a known script / checklist without deep debugging?


4. Automatable with our tools – Can it be done by:

Reading an endpoint

Scraping a web page

Running Splunk queries

Doing simple branching logic (if/else)?



5. Scales with load – As traffic/users grow, will this work increase?


6. Low “artistic value” – If a bot did it, would you not feel sad? (e.g. no loss of deep learning, no creative architecting.)



If Q1–4 are Yes, it’s almost certainly toil.
If Q5 is Yes too, it’s high-value toil.

Simple 5-point scoring (for prioritization)

Ask the engineer to score each dimension from 1–5:

Frequency (F)
1 = once a quarter, 5 = multiple times per day

Effort per run (E)
1 = < 2 minutes, 5 = ≥ 30 minutes

Repetitiveness (R)
1 = every run is different, 5 = steps are 90% identical

Cognitive depth (C)
1 = deep investigation, 5 = mostly mechanical

Automation feasibility (A) (with your workflow builder)
1 = lots of human judgment / external constraints
5 = pure data gathering + rule-based decisions via endpoints/web/Splunk


Then compute a Toil Priority Score:

> Score = F + E + R + A + (6 – C)
(Because higher “C” means less toil, so we invert it.)



Max score = 25.
Rule-of-thumb:

20–25 → Top-tier automation candidate (do soon)

15–19 → Good candidate, batch with others

< 15 → Maybe later, or partial automation only


You can run this once a quarter as a “Toil Census”.


---

C. Documentation Template – “Toil Automation Card”

This is the simple format to capture everything needed to turn a task into a workflow. Think of it as one page per candidate.

You can paste this into Confluence / Notion / Google Doc or even turn it into a form.


---

1. Basic Info

Title:
Short and action-based. Example: Auto-diagnose 5xx spikes for Consumer API

Owner:
Person/team that understands the flow.

Type:
Incident triage / Consumer request / Health check / Report / Other

Toil Priority Score:
(From the scoring above: e.g. 21/25)



---

2. Business Context

Why does this task exist?
(e.g. “When Consumer X sees errors, we manually check logs and endpoint health to respond quickly.”)

Who benefits?
(e.g. “L1 on-call, Consumer Success team, external partner FooCorp.”)

What happens if it’s delayed or missed?
(e.g. “Increased MTTR, more escalations, SLA breach risk after ~30 mins.”)



---

3. Trigger & Inputs

Trigger Event:
What starts this flow?

Example: “Alert: consumer_api_5xx_rate > 2% for 5m”

Or: “Inbound ticket with label LOG_DUMP_REQUEST”

Or: “Scheduled daily at 08:00 UTC”


Required Inputs / Parameters:

Example: consumer_id, time_range, env (prod/stage)


Where do inputs come from?

Alert payload / ticket fields / static config / consumer form.




---

4. Data Sources (aligned with your builder)

Specify exactly what the workflow will call/read.

HTTP Endpoints to call

URL: https://internal-api/foo/status?consumer_id={consumer_id}

Method: GET

Auth: Service token X

Expected response shape: short description or sample JSON snippet.


Web Pages to Scrape

URL: https://grafana.internal/d/abc123/consumer-dashboard

What to extract: “Current error rate, P95 latency numbers from the main panel.”


Splunk Queries

Query name: consumer_error_logs

SPL:

index=prod app=consumer_api consumer_id={{consumer_id}}
earliest=-15m latest=now error OR exception

What we care about in results: “Top 3 error signatures and count.”




---

5. Step-by-step Flow (Human Runbook Version)

Write it as you currently perform it, one step per line. This becomes the blueprint.

Example:

1. Receive alert or ticket with consumer_id.


2. Open API status endpoint, check status field.


3. If status != "OK", note error code/message.


4. Run Splunk query for last 15 minutes with consumer_id.


5. Count number of log entries with ERROR or Exception.


6. If number > 50, classify as “major issue”, else “minor issue”.


7. Prepare a short summary for Slack:

Current status

Number of errors

Top 1–2 error messages



8. Post summary in #consumer-support channel and update ticket.



You don’t need to design the workflow logic; just describe what a human does today. The workflow-builder can mirror this with condition blocks.


---

6. Decision Points & Branching

Call out your ifs explicitly:

Branch 1: Endpoint unhealthy

Condition: status != "OK"

Action: mark severity HIGH, include status message in summary.


Branch 2: Endpoint healthy but logs noisy

Condition: status == "OK" and error_count > 50

Action: mark severity MEDIUM, highlight potential downstream issue.


Branch 3: Everything looks normal

Condition: error_count <= 10

Action: respond with “no ongoing issue, please retry; share correlation ID if persists.”




---

7. Error Handling & Fallbacks

If endpoint call fails:

Retry 3 times with backoff.

If still failing, log failure and escalate to on-call.


If Splunk query fails:

Try alternative index/equivalent query.

If still failing, mention “log search unavailable” in summary.




---

8. Output & Notifications

What should the workflow produce?

Example: Slack message, ticket comment, email, dashboard entry.


Target Channels / Destinations:

Slack: #consumer-support, #oncall

Ticket system: comment on existing ticket

Status page: optional template.



Include a message template:

> Example Slack message template
Consumer: {consumer_id}
Status endpoint: {status}
Error count (last 15m): {error_count}
Top error: {error_signature}
Classification: {major/minor/none}




---

9. Success Metrics

Define what “good automation” looks like:

Time saved per run:
Manual: ~15 mins, Automated: ~1–2 mins.

Reduction in toil hours per month:
runs_per_month * (manual_time – automated_time)

Quality improvements:

Fewer missed checks

More consistent responses

Faster MTTR




---

2. Data Reasoning – Why this structure works

The question sets push engineers to think in terms of frequency, repeatability, and data availability (endpoints/web/Splunk) instead of vague “annoyances”.

The Toil Priority Score gives you a semi-quantitative way to:

Compare tasks across engineers.

Sort candidates in a backlog.


The template ensures you collect:

Triggers (what starts the workflow)

Inputs and data sources (what your builder can plug into)

Deterministic steps + decision points (what becomes logic)

Observability around the workflow itself (metrics, errors).



If you later want to get fancy, you can analyze:

Distribution of F/E/R per team.

Correlate top toil candidates with incident tags or SLO burn events.



---

3. Systems Analysis – Trade-offs & Feedback Loops

Good news: You reduce repetitive grunt work and MTTR.

Risk: If automation is opaque, people lose context and debugging skills.

Mitigate by making workflows transparent: each step logs its decisions and links back to underlying data (Splunk results, endpoint payload).


Risk: Teams may try to automate tasks that actually require judgment.

Mitigate with the Cognitive Depth part of the score and a quick design review.


Positive feedback loop:

Less toil → more time for better observability → better signals → better workflows → even less toil.


Negative feedback to watch:

If automations are flaky, you can create meta-toil (“toil to keep the toil-automation working”). Have clear owners and error budgets for critical workflows.




---

4. Validation & Simulation – How to test this approach

Once you’ve filled a few Toil Automation Cards:

1. Pull historical data

From ticketing: GROUP BY task_title / category to see top repeat offenders.

From Splunk: search for common manual queries in your “user logs” or audit logs.



2. Back-test candidates

For a sample month, ask: “If this workflow existed, how many times would it have run? How many engineer-hours would it have saved?”



3. Pilot one or two workflows

Implement the top 1–2 cards.

Track:

Execution success rate

Time per execution

Number of human escalations




4. Retrospective

After 1–2 months, review:

Did it really reduce toil, or just move it?

Are engineers trusting the workflow, or bypassing it?






---

5. Ops-Ready Summary (Checklist)

[ ] Run Toil Discovery session using the probing questions.

[ ] For each candidate task, fill Toil Qualification Checklist and compute score.

[ ] Select high-score items (≥ 20/25) as workflow backlog.

[ ] For each, create a Toil Automation Card:

Trigger & inputs

Data sources (endpoint / web / Splunk)

Step-by-step current runbook

Decisions, error handling, outputs

Success metrics


[ ] Implement 1–2 workflows first, instrument them, then iterate.


Once this is in place, you’ve basically given your team an internal “toil product framework”: discover → qualify → spec → automate → measure. Perfect soil for your workflow builder to grow in.